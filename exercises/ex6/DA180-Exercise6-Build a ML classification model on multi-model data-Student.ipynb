{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teched 2022 DA180 Workshop - Ex6 Build a classification model on multi-model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we want to apply SAP HANA Cloud multi-model processing techniques to prepare and assemble geo-located information about fuel stations in Germany, and make use of Machine Learning classification techniques from the [Predictive Analysis Library (PAL)](https://help.sap.com/docs/HANA_CLOUD_DATABASE/319d36de4fd64ac3afbf91b1fb3ce8de/c9eeed704f3f4ec39441434db8a874ad.html?locale=en-US) to model price-class categories for the fuels stations and deduct influence of station attributes incl. spatial attributes and their impact explaining the price-classes.\n",
    "\n",
    "This exercise builds on top of exercise 5, hence the exercise 5 tasks need to be completed before starting with this section.\n",
    "\n",
    "The __objective and goals__ for this exercise is\n",
    "\n",
    "- in ex 6.1 to build a fuel station price-class label variable based on average e5 fuel-price levels and build station master data- and price-indicator attributes dataframes\n",
    "- in ex 6.2 to an additional station attribute dataframe with multiple geo-location derived attributes\n",
    "- in ex 6.3 to build a station price-class classification model, review feature influence (esp. of spatial features) on the price-class labels.\n",
    "\n",
    "As an extra and optional exercise, the [add-on section](#ADDON)  describes how to evaluate, store and debrief the classifier model in more detail. Furthermore an add-on exercise describes how to download German-highway network data and calculate the spatial distance between stations and the next highway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation steps in your Python Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hana_ml\n",
    "print(hana_ml.__version__)\n",
    "from hana_ml import dataframe\n",
    "from hana_ml.dataframe import create_dataframe_from_pandas, create_dataframe_from_shapefile\n",
    "from hana_ml.algorithms.pal.tsa.additive_model_forecast import AdditiveModelForecast\n",
    "from hana_ml.algorithms.pal import metrics\n",
    "from hdbcli import dbapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to SAP HANA Cloud database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '[YourHostName]'\n",
    "port = 443\n",
    "user = '[YourUser]'\n",
    "passwd = '[YourUserPassword]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = dataframe.ConnectionContext(url, port, user, pwd)\n",
    "conn= dataframe.ConnectionContext(address=host, port=port, user=user, password=passwd,\n",
    "                               encrypt='true' ,sslValidateCertificate='false')\n",
    "conn.hana_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 6.1 - Prepare and explore fuel station classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1 - Create the classification label-column__  \n",
    "Using an advanced SQL statement, we define a HANA dataframe with the STATION_CLASS target column, deriving a station's fuel price class from it's average daily-average e5 price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a station price class type label \"STATION_CLASS\", derived from daily average e5-price\n",
    "stations_class=conn.sql(\n",
    "\"\"\"\n",
    "SELECT \"station_uuid\", E5_AVG, BINNING(VALUE => E5_AVG, BIN_COUNT => 10) OVER () AS STATION_CLASS \n",
    "  FROM ( SELECT  \"station_uuid\",  avg(\"e5\") AS E5_AVG \n",
    "         FROM (  SELECT \"station_uuid\", \"date\", \n",
    "\t                      EXTRACT(DAY FROM \"date\")||'-'||EXTRACT(MONTH FROM \"date\")||'-'||EXTRACT(YEAR FROM \"date\") AS \"DAY\", \n",
    "\t                      HOUR(\"date\") as HOUR, \n",
    "                          \"e5\", \"e5change\"\n",
    "                 FROM  \"GAS_PRICES\"  \n",
    "                 WHERE  \"e5\" > 1.3 AND \"e5\" < 2.8)\n",
    "         GROUP BY \"station_uuid\" \n",
    "         HAVING COUNT(\"e5\")>20)\n",
    "\"\"\"\n",
    ")\n",
    "stations_class.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2 - Create the station master attributes dataframe__  \n",
    "Create a HANA dataframe, with key master attributes of potential interest and relation to the price-level classification.  As the five digit postal code might be too detailed feature, we are abstracting it to the first two and three digits of the postal code, indicating more regional than local aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create station masterdata HANA dataframe: station_master\n",
    "stations_hdf = conn.table(\"GAS_STATIONS\")\n",
    "\n",
    "#print(\"There are\", stations_hdf.count(), \"service stations in Germany\")\n",
    "\n",
    "station_master=stations_hdf.select('uuid', 'brand', ('substr(\"post_code\",1,2)', 'post_code2'), \n",
    "                                   ('substr(\"post_code\",1,3)', 'post_code3'), 'city')\n",
    "display(station_master.head(3).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3 - Create the station price-level indicator attributes dataframe__  \n",
    "In this dataframe, based on a complex SQL statement, we derive some station-related e5-price indicator attributes like\n",
    "- *-E5C-D as daily \"e5change\"-count derived indicators (sum, min, ....)\n",
    "- *-E5-D as aggregates (AVG, SUM) over aggregated (VAR, STDDEV, MIN, ..) daily e5-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive some station-related e5-price indicators in a HANA dataframe: stations_price_indicators\n",
    "stations_price_indicators=conn.sql(\n",
    "\"\"\"\n",
    "SELECT \"station_uuid\", \n",
    "        /* Daily E5 change related indicators */\n",
    "        SUM(CAST(\"N_E5C_D\" as DOUBLE)) AS SUM_E5C, MIN(CAST(\"N_E5C_D\" as DOUBLE)) AS MIN_E5C, MAX(CAST(\"N_E5C_D\" as DOUBLE)) AS MAX_E5C, AVG(\"N_E5C_D\") AS AVG_E5C, \n",
    "        STDDEV(\"N_E5C_D\") AS STDEV_E5C, MAX(CAST(\"N_E5C_D\" as DOUBLE))-MIN(CAST(\"N_E5C_D\" as DOUBLE)) AS RANGE_E5C,\n",
    "        /* Daily E5 price related indicators */\n",
    "        AVG(\"VAR_E5_D\") AS AVG_E5_VAR,  AVG(\"STDDEV_E5_D\") AS AVG_E5_STD,  AVG(\"MIN_E5_D\") AS AVG_E5_MIN,  \n",
    "        AVG(\"MAX_E5_D\") AS AVG_E5_MAX, SUM(\"RANGE_E5_D\") AS SUM_E5_RANGE,  AVG(\"RANGE_E5_D\") AS AVG_E5_RANGE \n",
    "    FROM (\n",
    "          SELECT \"station_uuid\", DAY,  \n",
    "                /* Daily price analysis indicators */\n",
    "                count(\"e5change\") AS N_E5C_D, VAR(\"e5\") AS VAR_E5_D,  STDDEV(\"e5\") AS STDDEV_E5_D,  MIN(\"e5\") AS MIN_E5_D, \n",
    "                MAX(\"e5\") AS MAX_E5_D, AVG(\"e5\") AS AVG_E5_D, MAX(\"e5\")-MIN(\"e5\") AS RANGE_E5_D\n",
    "          FROM (\n",
    "                  SELECT \"station_uuid\", \"date\", \n",
    "\t                      EXTRACT(DAY FROM \"date\")||'-'||EXTRACT(MONTH FROM \"date\")||'-'||EXTRACT(YEAR FROM \"date\") AS \"DAY\", \n",
    "\t                      HOUR(\"date\") as HOUR, \n",
    "                          \"e5\", \"e5change\"\n",
    "                  FROM  \"GAS_PRICES\"  \n",
    "                  WHERE  \"e5\" > 1.3 AND \"e5\" < 2.8)\n",
    "          GROUP BY \"station_uuid\", DAY)\n",
    "    GROUP BY \"station_uuid\";\n",
    "\"\"\"\n",
    ")\n",
    "stations_price_indicators.head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is the potential risk of information leakage from indicators to the model target, if any of the indicators show a high correlation with the target column price_class (which has been derived from average price-class levels), we want evaluate the numeric feature correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate correlation of indicators with AVG(e5) and thus target class\n",
    "\n",
    "# Join indicator columns with AVG(e5)\n",
    "stations_num=stations_price_indicators.set_index(\"station_uuid\").join(\n",
    "             stations_class.drop('STATION_CLASS').set_index(\"station_uuid\"))\n",
    "\n",
    "# Avoid usage of intercorrelated indicators, e.g. correlate numerical columns\n",
    "import matplotlib.pyplot as plt\n",
    "from hana_ml.visualizers.eda import EDAVisualizer\n",
    "f = plt.figure(figsize=(15, 8))\n",
    "ax1 = f.add_subplot(111)\n",
    "eda = EDAVisualizer(ax1)\n",
    "ax1, corr = eda.correlation_plot(data=stations_num.drop('station_uuid'), cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to high correlation with the \"E5_AVG\" column and intercorrelations (values higher than 0.5), we will exclude a number of features from the price indicators_dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop high correlated indicator columns\n",
    "stations_price_indicators=stations_price_indicators.drop('AVG_E5_MIN').drop('AVG_E5_MAX').drop('SUM_E5C').drop('AVG_E5C')\n",
    "stations_price_indicators=stations_price_indicators.drop('AVG_E5_STD').drop('SUM_E5_RANGE').drop('AVG_E5_RANGE')\n",
    "stations_price_indicators.head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 6.2 - Enrich fuel station classification data with spatial attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we will define a __data-driven spatial hierarchy__ from the stations point locations using its geohashes. \n",
    "- __Geohashes__ are unique hash-values derived from geo-locations, by ommitting trailing values from the 20 character hash-string. Simply a rectangle represented by the first 5 characters of the geohash, is guaranteed to contain to any rectangle / point represented by the same first 5 + n characters geohash.\n",
    "- The __generated_feature__ dataframe-function allows to generate a hierarchy geohash features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a station spatial hierarchy HANA dataframe\n",
    "stations_spatialhierarchy = stations_hdf.select('uuid', 'longitude','latitude','longitude_latitude_GEO')\n",
    "\n",
    "# Derive spatial hierarchy features from station point location\n",
    "stations_spatialhierarchy =stations_spatialhierarchy.generate_feature(targets='longitude_latitude_GEO', \n",
    "                                                          trans_func='GEOHASH_HIERARCHY', trans_param=range(3,6))\n",
    "\n",
    "# Rename columns\n",
    "stations_spatialhierarchy=stations_spatialhierarchy.rename_columns({'GEOHASH_HIERARCHY(longitude_latitude_GEO,3)': 'GEO_H3', \n",
    "                                                                    'GEOHASH_HIERARCHY(longitude_latitude_GEO,4)': 'GEO_H4', \n",
    "                                                                    'GEOHASH_HIERARCHY(longitude_latitude_GEO,5)': 'GEO_H5'\n",
    "                                                                   }\n",
    "                                                                      )\n",
    "\n",
    "stations_spatialhierarchy.head(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stations closest distance to the network of highways in Germany as been pre-calcualted and can be imported from the file stations_hwaydist.csv\n",
    "- Note in the [add-on exercises section](#ADDON_OSMNX) it is described, how the Germany highway network can be imported using the OpenStreetMap network Python interface OSMNX, imported as HANA graph, a Multilinestring created for the complete highway network and the distance calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import station distance to nearest highway and join with spatial hierarchy data\n",
    "stations_hwaydist_pd = pd.read_csv('./data/fuelprice/stations_hwaydist.csv', sep=',', header=0, skiprows=1,\n",
    "                                      names=[\"idx\", \"uuid\", \"HIGHWAY_DISTANCE\"],\n",
    "                                      usecols=[\"uuid\", \"HIGHWAY_DISTANCE\"])\n",
    "stations_hwaydist = create_dataframe_from_pandas(\n",
    "        conn,\n",
    "        stations_hwaydist_pd,    \n",
    "        table_name=\"GAS_STATION_HWAYDIST\",\n",
    "        force=True,\n",
    "        replace=True,\n",
    "        drop_exist_tab=True,\n",
    "        table_structure={\"uuid\": \"NVARCHAR(5000)\", \"HIGHWAY_DISTANCE\": \"DOUBLE\"}\n",
    "    )\n",
    "display(stations_hwaydist.head(5).collect())\n",
    "\n",
    "# Joining distance data with spatial hierachy dataframe \n",
    "stations_spatial_attributes=stations_spatialhierarchy.set_index(\"uuid\").join(stations_hwaydist.set_index(\"uuid\"))\n",
    "display(stations_spatial_attributes.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we spatially join the stations spatial attributes dataframe with the German-region \"Landkreise\" shape and select __relevant region attributes__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create station spatial dataframe joining spatial hierachy and regions attributes\n",
    "regions_hdf = conn.table(\"GEO_GERMANY_REGIONS\")\n",
    "\n",
    "# Joins regions and stations via HANA spatial join-function\n",
    "stations_spatial = stations_spatial_attributes.join(regions_hdf.select('lan_name','krs_name','krs_type','SHAPE'), \n",
    "       '\"longitude_latitude_GEO\".ST_SRID(25832).st_transform(25832).st_intersects(SHAPE)=1')\n",
    "\n",
    "stations_spatial.drop('SHAPE').head(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 6.3 - Build fuel station classification model and evaluate impact of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1 - Create the station classification HANA dataframe__  \n",
    "For conveniance, we first save the station dataframes as local temporary tables in SAP HANA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save station attributes dataframes to temporary HANA tables\n",
    "stations_class.drop('E5_AVG').save('#STATION_CLASS', force=True)\n",
    "station_master.save('#STATION_MASTER', force=True)\n",
    "stations_price_indicators.save('#STATION_PRICE_INDICATORS', force=True)\n",
    "stations_spatial.drop('longitude_latitude_GEO').drop('SHAPE').save('#STATION_SPATIAL', force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an advance join SQL statement of all the attributes temporary table, we create the stations priceclass dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classification training data HANA dataframe\n",
    "stations_priceclass=conn.sql(\n",
    "\"\"\"\n",
    "SELECT M.\"uuid\", \"brand\", \"post_code2\", \"post_code3\", \"city\", \n",
    "       \"longitude\", \"latitude\", \"GEO_H3\", \"GEO_H4\", \"GEO_H5\", \"lan_name\", \"krs_name\", \"krs_type\",\n",
    "       S.HIGHWAY_DISTANCE,\n",
    "       \"MIN_E5C\", \"MAX_E5C\", \"STDEV_E5C\", \"RANGE_E5C\", \"AVG_E5_VAR\",  \n",
    "       \"STATION_CLASS\"\n",
    "   From #STATION_MASTER as M,\n",
    "        #STATION_SPATIAL as S,\n",
    "        #STATION_PRICE_INDICATORS as PI,\n",
    "        #STATION_CLASS as C\n",
    "    Where M.\"uuid\"=PI.\"station_uuid\" AND \n",
    "          M.\"uuid\"=S.\"uuid\" AND\n",
    "          M.\"uuid\"=C.\"station_uuid\";\n",
    "\"\"\"\n",
    ")\n",
    "stations_priceclass.head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we then save to as our model development base dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save station classification dataset to HANA column table\n",
    "stations_priceclass.save('STATION_PRICECLASSIFICATION', force=True)\n",
    "\n",
    "gas_station_class_base = conn.table(\"STATION_PRICECLASSIFICATION\")\n",
    "gas_station_class_base.head(5).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2 - Splitting up our base dataset__  \n",
    "In order to validate evaluate our classification model throughout training iterations and neutrally test it after the training efforts have been completed, we split up or base data set into train-, validation- and test-subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the station classification dataframe into a training and test subset\n",
    "from hana_ml.algorithms.pal.partition import train_test_val_split\n",
    "df_train, df_test, df_val = train_test_val_split(data=gas_station_class_base, id_column='uuid',\n",
    "                                            random_seed=2, partition_method='stratified', stratified_column='STATION_CLASS',\n",
    "                                            training_percentage=0.70,\n",
    "                                            testing_percentage=0.15,\n",
    "                                            validation_percentage=0.15)\n",
    "\n",
    "df_train.describe().collect()\n",
    "#print(df_train.describe().select_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the describe column report of our training subset shows, some of the columns contains null values (brand, post_code). During model training, we will take of this.   \n",
    "  \n",
    "In order to evaluate the classification thoughout the training interations, we union the train- and validation subsets so we can pass both to the training step. The subsets beeing indicated by the TRAIN_VAL_INDICATOR column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union train and validation data into one set\n",
    "df_trainval=df_train.select('*', ('1', 'TRAIN_VAL_INDICATOR' )).union(df_val.select('*', ('2', 'TRAIN_VAL_INDICATOR' )))\n",
    "\n",
    "display(df_trainval.head(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3 - Train the station price-class classification model__  \n",
    "We now use the [PAL Unified Classification method](https://help.sap.com/doc/1d0ebfe5e8dd44d09606814d83308d4b/latest/en-US/hana_ml.algorithms.pal_algorithm.html?highlight=unified%20classification#module-hana_ml.algorithms.pal.unified_classification), to train a [HybridGradientBoostingTree](https://help.sap.com/docs/HANA_CLOUD_DATABASE/319d36de4fd64ac3afbf91b1fb3ce8de/ca5106cbd88f4ac69e7538bbc6a059ea.html?locale=en-US) classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Station classifer model using PAL HybridGradientBoostingTree\n",
    "from hana_ml.algorithms.pal.unified_classification import UnifiedClassification\n",
    "\n",
    "# Initialize the model object \n",
    "hgbc = UnifiedClassification(func='HybridGradientBoostingTree',\n",
    "                            n_estimators = 101, split_threshold=0.1,\n",
    "                            learning_rate=0.5, max_depth=5,\n",
    "                            resampling_method='cv',fold_num=5, ref_metric=['auc'],\n",
    "                            evaluation_metric = 'error_rate',\n",
    "                            thread_ratio=1.0)\n",
    "\n",
    "# Execute the training of the model\n",
    "hgbc.fit(data=df_trainval, key= 'uuid',\n",
    "         label='STATION_CLASS', categorical_variable='STATION_CLASS',\n",
    "         impute=True, strategy='most_frequent-mean',\n",
    "         ntiles=20,  build_report=True,\n",
    "         partition_method='user_defined', purpose='TRAIN_VAL_INDICATOR' )\n",
    "\n",
    "display(hgbc.runtime)\n",
    "\n",
    "# Explore the feature importance result\n",
    "# display(hgbc.importance_.sort('IMPORTANCE', desc=True).collect().set_index('VARIABLE_NAME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4 - Explore the model report and attribute importance__  \n",
    "Using the __model report__, we can now explore training and validation model performance statistics, confusion matrix, explore feature importance and classification metric reports like ROC, gain or lift charts.  \n",
    "Finally, looking the __feature importance__-section of the report, the relative importance of all attributes explaining and contributing to the models global classification performance is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build Model Report\n",
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "UnifiedReport(hgbc).build().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Applying the __model PREDICT-method__ to station data (e.g from our test-data subset), beside the models predicted classification for a station, we can review a __station's attributes value importance with respect to the predicted classification__ (local feature importance or explainability) in the REASON_CODE column output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore test data-subset predictions applying the trained model\n",
    "features = df_test.columns\n",
    "features.remove('STATION_CLASS')\n",
    "features.remove('uuid')\n",
    " \n",
    "# Using the PREDICT-method with our model object hgbc\n",
    "pred_res = hgbc.predict(df_test.head(1000), \n",
    "                        key='uuid', features=features, impute=True, \n",
    "                        thread_ratio=1.0)\n",
    "\n",
    "display(hgbc.runtime)\n",
    "\n",
    "# Review the predicted results\n",
    "pd.set_option('max_colwidth', None)\n",
    "pred_res.select('uuid', 'SCORE', 'CONFIDENCE', 'REASON_CODE', \n",
    "                ('json_query(\"REASON_CODE\", \\'$[0].attr\\')', 'Top1'), \n",
    "                ('json_query(\"REASON_CODE\", \\'$[0].pct\\')', 'PCT_1') ).head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Add-on exercises<a id='ADDON'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score and debrief model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Unified Classification-SCORE method, we can benchmark and test our models generalization against data completely unseen during model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generalization using the test data-subset, not used during training\n",
    "scorepredictions, scorestats, scorecm, scoremetrics = hgbc.score(data=df_test.head(100) , key= 'uuid', label='STATION_CLASS', \n",
    "                                                                 ntiles=20, impute=True,\n",
    "                                                                 thread_ratio=1.0)\n",
    "#display(hgbc.runtime)\n",
    "display(scorestats.sort('CLASS_NAME').collect())\n",
    "display(scorecm.filter('COUNT != 0').collect())\n",
    "#display(scoremetrics.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapley explainer plot, shows the distribution each feature has on the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Show the distribution of the impacts each feature has on the model output using Shapley ML explainability values \n",
    "import pydotplus\n",
    "import graphviz\n",
    "from hana_ml.visualizers.model_debriefing import TreeModelDebriefing\n",
    "\n",
    "shapley_explainer = TreeModelDebriefing.shapley_explainer(pred_res.head(1000), df_test.head(1000), \n",
    "                                                          key='uuid', label='STATION_CLASS')\n",
    "shapley_explainer.summary_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model and retrieve stored models and model reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ModelStorage methods, we can store and retrieve models and model performance reports from a given storage schema.\n",
    "- Note, using the schema=-option in the ModelStorage definition, it can be determined on where to save the models. Here, adjust the schema to your  connection database userid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate ModelStorage location\n",
    "from hana_ml.model_storage import ModelStorage\n",
    "MLLAB_models = ModelStorage(connection_context=conn, schema=\"TECHED_USER_###\")\n",
    "\n",
    "#Describe and save current model\n",
    "hgbc.name = 'STATION PRICE-CLASS CLASSIFIER MODEL' \n",
    "hgbc.version = 1\n",
    "\n",
    "MLLAB_models.save_model(model=hgbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model from ModelStorage location\n",
    "from hana_ml.model_storage import ModelStorage\n",
    "MLLAB_models = ModelStorage(connection_context=conn)\n",
    "\n",
    "list_models = MLLAB_models.list_models()\n",
    "display(list_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a later point in time, we can reconnect to the ModelStorage schema, retrieve stored models and revisit model reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve model from ModelStorage location\n",
    "from hana_ml.model_storage import ModelStorage\n",
    "MLLAB_models = ModelStorage(connection_context=conn, schema=\"TECHED_USER_###\")\n",
    "\n",
    "# Reload model from ModelStorage\n",
    "mymodel = MLLAB_models.load_model('STATION PRICE-CLASS CLASSIFIER MODEL', 1)\n",
    "\n",
    "# Predict with reloaded model\n",
    "pred_results=mymodel.predict(data=df_test.head(10), key='uuid', features=features, impute=True)\n",
    "\n",
    "# Build Model Report\n",
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "UnifiedReport(mymodel).build().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As needed, models and the complete model storage can be deleted as needed and database authorizations allow for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CleanUp Model Storage\n",
    "MLLAB_models.delete_models(name='STATION PRICE-CLASS CLASSIFIER MODEL')\n",
    "MLLAB_models.clean_up()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use OSMNX to import German Highway network and calculate spatial distance btw station and next highway<a id='ADDON_OSMNX'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OSMNX is a Python package that lets you download geospatial data from OpenStreetMap and model, project, visualize, and analyze real-world street networks and any other geospatial geometries. You can download and model walkable, drivable, or bikeable urban networks with a single line of Python code then easily analyze and visualize them. You can just as easily download and work with other infrastructure types, amenities/points of interest, building footprints, elevation data, street bearings/orientations, and speed/travel time.See https://osmnx.readthedocs.io/en/stable/index.html for reference details.\n",
    "\n",
    "There are multiple methods to download street network data, like  \"graph from place\" or \"graph from polygon\".  \n",
    "! Be careful, downloading the german highway network via OSMNX takes multiple hours.  \n",
    "! There if you seek to explore this section, try out the next step and download only the highway network for a single region instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import osmnx as ox\n",
    "\n",
    "# Use OSMNX-method to download network graph from \"place\"\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "cf = '[\"highway\"~\"motorway\"]'\n",
    "\n",
    "# !!careful, downloading the german highway network via OSMNX takes multiple hours\n",
    "# If you want to try this out, try out the next step instead download the highway network for a single region\n",
    "\n",
    "#g =  ox.graph_from_place('Germany', network_type = 'drive', custom_filter=cf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Use OSMNX to download only the highway network for a single region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import osmnx as ox\n",
    "hdf_RNK_SHAPE = stations_spatial.filter(\"\\\"krs_name\\\"='Landkreis Rhein-Neckar-Kreis'\" ).select('uuid','krs_name', 'SHAPE').head(1)\n",
    "display(hdf_RNK_SHAPE.drop('SHAPE').collect())\n",
    "\n",
    "# Create Geopandas Dataframe from the HANA dataframe\n",
    "gdf_RNK_SHAPE = gpd.GeoDataFrame(hdf_RNK_SHAPE.select('uuid', 'SHAPE').collect(), geometry='SHAPE')\n",
    "gdf_RNK_SHAPE=gdf_RNK_SHAPE.rename_geometry('geometry')\n",
    "display(gdf_RNK_SHAPE.head(10))\n",
    "\n",
    "# Use OSMNX-method to download network graph from polygon\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "cf = '[\"highway\"~\"motorway\"]'\n",
    "g = ox.graph_from_polygon(polygon = gdf_RNK_SHAPE['geometry'][0], network_type = 'drive',custom_filter=cf)\n",
    "#fig, ax = ox.plot_graph(g, fig_height=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the network graph object has been successfully downloaded and stored as dataframe \"g\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check successful object download\n",
    "g\n",
    "\n",
    "# Plot OSMNX highway network graph data\n",
    "fig, ax = ox.plot_graph(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load graph nodes and edges into geopandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframes from network graph\n",
    "gdf_nodes,gdf_edges = ox.graph_to_gdfs(g, nodes=True, edges=True)\n",
    "gdf_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the graph dataframes into SAP HANA, the network arrays from the geopandas dataframe require to be str-column converted into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert network arrays to str-column format for the pandas dataframe\n",
    "gdf_edges['ID'] = np.arange(len(gdf_edges))\n",
    "gdf_edges['osmid']=gdf_edges['osmid'].astype(str)\n",
    "gdf_edges['ref']=gdf_edges['ref'].astype(str)\n",
    "gdf_edges['highway']=gdf_edges['highway'].astype(str)\n",
    "\n",
    "# Create a pandas dataframe, needed for the HANA dataframe import\n",
    "pd_edges=pd.DataFrame(gdf_edges, copy=True)[['ID', 'osmid', 'geometry', 'highway','ref']]\n",
    "pd_edges.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the HANA dataframe for the street network edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HANA dataframe from the German highway-network edges pandas dataframe \n",
    "from hana_ml.dataframe import create_dataframe_from_pandas\n",
    "\n",
    "hdf = create_dataframe_from_pandas(\n",
    "    connection_context=conn, replace=True,\n",
    "    pandas_df=pd_edges,\n",
    "    geo_cols=[\"geometry\"],\n",
    "    srid=4326,\n",
    "    table_name=\"GEO_GERMANY_HIGHWAYS\", primary_key='ID'\n",
    "    , drop_exist_tab=True, force=True)\n",
    "\n",
    "german_highways = conn.sql('select * from \"GEO_GERMANY_HIGHWAYS\"')\n",
    "german_highways.head(3).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the distance btw station and Germany highway network, we create a list or German region \"Landkreis\" area, so we calculate the distance in batches of stations within a regional area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=stations_spatial.distinct('krs_name').collect()\n",
    "krs=list(set(list(df['krs_name'])))\n",
    "print(sorted(krs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this SQL statements\n",
    "- we precalculate a single multilinestring for the complete German highway network\n",
    "- then calculate the distance between the multilinestring and the station points using the HANA spatial function ST_DISTANCE, we select the stations in batches of \"Landkreis\" regions\n",
    "- store the calculated disctance to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "\n",
    "#Calculate single Highway-Multilinestring from Highway network into temporary table #HWL\n",
    "CREATE LOCAL TEMPORARY COLUMN TABLE #HWL ( HIGHWAY NVARCHAR(24), HIGHWAY_LINE ST_GEOMETRY(4326));\n",
    "--#ALTER TABLE HWAY ALTER (HIGHWAY_LINE ST_GEOMETRY(4326));\n",
    "INSERT INTO #HWL\n",
    "\tSELECT \"highway\",  NEW ST_MultiLineString('MultiLineString (' || substring(LSTRING,3) || ')', 4326) AS HIGHWAY_LINE\n",
    "\t\t\t\tFROM (\n",
    "\t\t\t\t\t\tSELECT \"highway\", replace(agg , 'SRID=4326;LINESTRING ', ', ') AS LSTRING\n",
    "\t\t\t\t\t\tFROM (\n",
    "\t\t\t\t\t\t\t\tSELECT \"highway\",  STRING_AGG(\"geometry_GEO\") AS agg \n",
    "\t\t\t\t\t\t\t\tFROM GEO_GERMANY_HIGHWAYS \n",
    "\t\t\t\t\t\t\t\tWHERE substr(\"ref\",1,1)='A' AND \"highway\"='motorway'\n",
    "\t\t\t\t\t\t\t\tGROUP BY \"highway\"\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t);\n",
    "        \n",
    "CREATE COLUMN TABLE STATION_HWAYDIST (\"uuid\" NVARCHAR(5000), HIGHWAY_DISTANCE DOUBLE);\n",
    "INSERT INTO STATION_HWAYDIST \n",
    "SELECT \"uuid\", \"STATION_P\".ST_SRID(1000004326).ST_DISTANCE(HIGHWAY_LINE.ST_SRID(1000004326), 'meter') AS HIGHWAY_DISTANCE\n",
    "from\n",
    "\t(SELECT \"uuid\", \"longitude_latitude_GEO\".ST_SRID(4326) AS \"STATION_P\" \n",
    "   \t\tFROM \t(SELECT S.\"uuid\", \"longitude_latitude_GEO\"\n",
    "         \t\tfrom STATION_PRICECLASSIFICATION S, GAS_STATIONS G\n",
    "         \t\tWHERE S.\"krs_name\" in (\n",
    " 'Kreis Borken', 'Kreis Coesfeld', 'Kreis Dithmarschen', 'Kreis Düren', 'Kreis Ennepe-Ruhr-Kreis', 'Kreis Euskirchen', \n",
    " 'Kreis Gütersloh', 'Kreis Heinsberg', 'Kreis Herford', 'Kreis Herzogtum Lauenburg', 'Kreis Hochsauerlandkreis', \n",
    "  ...\n",
    "  'Landkreis Würzburg', 'Landkreis Zollernalbkreis', 'Landkreis Zwickau', 'Stadtkreis Baden-Baden', \n",
    "  'Stadtkreis Freiburg im Breisgau', 'Stadtkreis Heidelberg', 'Stadtkreis Heilbronn', 'Stadtkreis Karlsruhe', \n",
    "  'Stadtkreis Mannheim', 'Stadtkreis Pforzheim', 'Stadtkreis Stuttgart', 'Stadtkreis Ulm'\n",
    "         \t\t) AND\n",
    "         \t\t       S.\"uuid\"=G.\"uuid\"\n",
    "         \t\t) AS P\n",
    "     ),\n",
    "     #HWL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can review the station-highway distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_hwaydist=conn.table(\"STATION_HWAYDIST\")\n",
    "display(stations_hwaydist.head(5).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
